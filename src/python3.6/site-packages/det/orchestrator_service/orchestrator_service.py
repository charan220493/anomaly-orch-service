
#stdlib imports
import sys
from json import load as _load, dumps as _dumps, loads as _loads, decoder as _decoder
from logging import getLogger as _getLogger, Formatter as _Formatter
from argparse import ArgumentParser as _ArgumentParser
import glob as _glob
from copy import deepcopy as _deepcopy
from os import getenv as _getenv, path as _path
from traceback import print_exc as _print_exc
# Import smtplib for the actual sending function
from smtplib import SMTP as _SMTP
# Import the email modules we'll need
from email.mime.text import MIMEText as _MIMEText
from calendar import timegm as _timegm
from time import gmtime as _gmtime, sleep as _sleep
from uuid import uuid4 as _uuid4



#3rd party imports
from pandas import to_numeric as _to_numeric, DataFrame as _DataFrame
from flask import Flask as _Flask, has_request_context as _has_request_context, request as _request, g as _g
from influxdb import InfluxDBClient as _InfluxDBClient
from requests import get as _get, post as _post
from regex import sub as _sub, compile as _compile

#internal imports
from det.dataframe.Transform import readData as _readData, decodeData as _decodeData, processInputData as _processInputData
from det.logging.logging_framework import setup_logging as _setup_logging

_setup_logging()

class RequestFormatter(_Formatter):
	def format(self, record):
		record.url = _request.url if  _has_request_context() else 'NoCtx'
		record.remote_addr = _request.remote_addr if  _has_request_context() else 'NoCtx'
		record.requestId = _g.requestId if  _has_request_context() and 'requestId' in _g else 'NotAssigned'
		return super().format(record)

formatter = RequestFormatter(
	'[%(asctime)s] %(remote_addr)s rid=%(requestId)s: '
	'%(levelname)s in %(module)s: message=%(message)s'
)
app = _Flask(__name__)

config_key = 'APP_CFG'
globalRoutingService = {}
if _getenv(config_key, None) is not None:
	app.config.from_envvar(config_key)
	if app.config.get('METASERVICE') is not None:
		with open(app.config['METASERVICE'],'r') as f:
			print('In Load using %s to configure initial grs'%(app.config.get('METASERVICE')))
			try:
				globalRoutingService = _loads(f.read())
				app.config.update(dict(
						GLOBAL_ROUTING_SERVICE=globalRoutingService
				))
			except _decoder.JSONDecodeError as e:
				print('fatal json decode error, exiting')
				raise e
			print('initial grs'%(app.config.get('GLOBAL_ROUTING_SERVICE')))
	else:
		print('Fatal error, routing table not found'%())
		raise Exception('MissingRoutingTable')
	requiredAppConfigParameters = ['STATS_STORAGE_SERVICE_ENDPOINT','ANOMALY_SERVICE_ENDPOINT','INFLUX_VISUALIZATION_SERVICE_ENDPOINT','INFLUX_MAX_DATASET_SIZE','MAX_DATASET_ROW_SIZE','MAX_DATASET_COLUMN_SIZE', 'MAX_DATASET_FIELD_SIZE','MAX_DATASET_COLUMN_SIZE_FOR_ALARM_STORAGE','ASYNC_WAIT_TIME']
	for elem in ['STATS_STORAGE', 'ANOMALY', 'VISUALIZATION']:
		requiredAppConfigParameters.append('%s_SERVICE_OPEN'%(elem))
	for elem in requiredAppConfigParameters:
		if app.config.get(elem) is None:
			print('Fatal error, required config parameter not present:%s'%(elem))
			raise Exception("MissingConfigParameter")
		else:
			print('config parameter %s set to: %s'%(elem,app.config.get(elem)))

else:
	print('NO CONFIG FOUND')
logger = _getLogger()
logger.info('module level initialization of logger formatter')
for handler in logger.handlers:
	print('setting found handler formatter %s'%(str(handler)))
	handler.setFormatter(formatter)

@app.route('/orchestrator_mainentry', methods=['GET', 'POST'])
def orchestrator_mainentry():
	logger = _getLogger(__name__)
	logger.info('in orchestrator_mainentry')

	message = "Dashboards not created"
	tableName = "Unknown"
	dashboardMessage = "Dashboards not created"
	outputPayload = {}
	outputPayload['status_code'] = 509 #default 
	outputPayload['status_text'] = 'ERROR' #default 
	outputPayload['status_detail_text'] = '' #initialize the string field for incremental updates to this field
	oneCSOutput = ''
	detailSteps = []
	try:
		commonHeaders = {}
		if 'requestId' not in _g:
			logger.info('assigning requestId')
			_g.requestId = str(_uuid4())
			logger.info('requestId is %s'%(_g.requestId))
		else:
			logger.info('using previously requestId %s'%(_g.requestId))
		commonHeaders['X-CS-Request-Id'] = _g.requestId

		with open(app.config['METASERVICE'],'r') as f:
			logger.info('using %s to configure grs'%(app.config.get('METASERVICE')))
			try:
				globalRoutingService = _loads(f.read())
			except _decoder.JSONDecodeError as e:
				logger.error("%s"%(str(e)))
				globalRoutingService = _deepcopy(app.config.get('GLOBAL_ROUTING_SERVICE'))
				logger.info('revert to globalRoutingService loaded at startup:%s'%(DictionaryAsString(globalRoutingService, maxLength=1000)))

		requiredParameters = ['version',  'business_id', 'group_id', 'database', 'cadence','input_data',"isFilePath"]

		parametersPresent = True
		if _request.method == 'POST':
			logger.info('accepting post request, perform parameter checks')
			logger.info('form k/v parameters:%s'%(DictionaryAsString(dict(_request.form.items()), maxLength=1000)))
			for elem in requiredParameters:
				if _request.form.get(elem) == None:
					parametersPresent = False
					outputPayload['status_code'] = 422
					outputPayload['version'] = '1'
					if outputPayload.get('status_text') is None:
						outputPayload['status_text'] = 'Missing param:%s'%(elem)
					else:
						outputPayload['status_text'] += ', Missing param:%s'%(elem)
			if not parametersPresent:
				detailSteps.append('missing parameters')
			else:
				if _request.form.get('version') == '1':
					logger.debug('after version check, before byte conversion')
					logger.debug('%s'%(str(_request.headers)))
					try:
						specificRoutingTable = globalRoutingService[_request.form['business_id']][_request.form['group_id']][_request.form['database']][_request.form['cadence']].copy()
						detailSteps.append("initial form parametrs: %s"%(DictionaryAsString(_request.form, maxLength=100)))
						detailSteps.append("initial routing table: %s"%(specificRoutingTable))
						specificRoutingTable['business_id'] = _request.form['business_id']
						specificRoutingTable['group_id'] = _request.form['group_id']
						specificRoutingTable['cadence'] = _request.form['cadence']
						#process overrides
						for elem in specificRoutingTable:
							newElem = _request.form.get(elem)
							if newElem is not None:
								#then this override is present, put into dictionary
								logger.info('overriding k,v %s,%s with %s'%(elem,specificRoutingTable[elem],newElem ))
								specificRoutingTable[elem] = newElem

					except KeyError as e:
						logger.error('Key Error accessing globalRoutingService %s'%(str(e)))
						raise(e)
					try:
						logger.info('Routing info:%s'%(str(specificRoutingTable)))
						tableName = '%s.%s.%s.%s'%(specificRoutingTable['business_id'],specificRoutingTable['group_id'],specificRoutingTable['tableName'],specificRoutingTable['cadence'])
						#for trakcing purposes read in row count here and append to otsdb metric
						#to put data into otsdb with tags the caller (orchestrator_mainentry) must know the tags to keep.  
						#read in the tags to keep here
						logger.info('read input data')
						encoding = specificRoutingTable['encoding']
						recordSeparator = specificRoutingTable['record_separator']
						fieldSeparator = specificRoutingTable['field_separator']
						input_data = _request.form['input_data']
						isFilePath = _request.form['isFilePath']
						logger.info('read input data file ')
						if int(isFilePath) == 1:
							inputa = _readData(input_data)
							decodedInputData = _decodeData(inputa, encoding)
						else:
							decodedInputData = input_data
						if len(decodedInputData) > 0: 
							data = _processInputData(decodedInputData, recordSeparator = recordSeparator, fieldSeparator = fieldSeparator)
							valueColumns = data['value_columns']
							groupColumns = data['group_columns']
							timeColumns = data['time_columns']
							try:
								datasetRowCount = data['df'].shape[0]
								aggregateColumnCount = len(groupColumns)
								fieldColumnCount = len(valueColumns)
							except Exception as e:
								type_, value_, traceback_ = sys.exc_info()
								#not enough paraeters to construct influx call
								logger.exception('caught exception %s'%(str(e)))
								logger.info('could not obtain row & column counts for data set')
								raise e
							#register this bid/gid into metadata.alpha metric_title_registry
							registrationEndPoint = specificRoutingTable['opentsdbEndPoints'].split(',')[0]
							value = 0
							jOTSDB = {'metric':'metadata.alpha','timestamp':1, 'value':0, 'tags':{'key':'metric_title_registry','value':tableName}}
							logger.info("registering %s into %s"%(tableName, jOTSDB))
							p1 = _post('%s/api/put/'%(registrationEndPoint), json=jOTSDB)
							logger.info("response is %s"%(p1.status_code))
							detailSteps.append('registration response: %d'%(p1.status_code))
							if p1.status_code >= 200 and p1.status_code <300:
								pass
							else:
								logger.warn("registration unsuccessful")
								raise Exception("FailedMetadata.alphaRegistration")
							#also insert record to indicate another processing request
							utcTime = _timegm(_gmtime())
							jOTSDB = {'metric':'metadata.alpha','timestamp':utcTime, 'value':datasetRowCount, 'tags':{'key':'orchestrator_invocation','value':tableName}}
							logger.info("usage metering and audit: %s into %s"%(tableName, jOTSDB))
							p1 = _post('%s/api/put/'%(registrationEndPoint), json=jOTSDB)
							logger.info("response is %s"%(p1.status_code))
							detailSteps.append('audit response: %d'%(p1.status_code))
							if p1.status_code >= 200 and p1.status_code <300:
								pass
							else:
								logger.warn("Auditing Failed")
								raise Exception("FailedMetadata.alphaAudit")

							#check data size.  if too big, then do not process
							if datasetRowCount > app.config.get('MAX_DATASET_ROW_SIZE'):
								logger.warn("Max dataset row count exceeded, halt processing")
								detailSteps.append('Max dataset row count exceeded , halt processing max size is %d'%(app.config.get('MAX_DATASET_ROW_SIZE')))
								raise Exception("DatasetSizeConstraint")
							if aggregateColumnCount > app.config.get('MAX_DATASET_COLUMN_SIZE'):
								logger.warn("Max dataset column count exceeded, halt processing")
								detailSteps.append('Max dataset column count exceeded , halt processing max size is %d'%(app.config.get('MAX_DATASET_COLUMN_SIZE')))
								raise Exception("DatasetSizeConstraint")
							if fieldColumnCount > app.config.get('MAX_DATASET_FIELD_SIZE'):
								logger.warn("Max dataset field count exceeded, halt processing")
								detailSteps.append('Max dataset field count exceeded , halt processing max size is %d'%(app.config.get('MAX_DATASET_FIELD_SIZE')))
								raise Exception("DatasetSizeConstraint")
							if min(data['df'].timestamp) < 10000000:
								logger.warn("minimum timestamp is < 10000000 seconds, potential issue with time column")
							#call stats storage
							statsStorage = app.config.get('STATS_STORAGE_SERVICE_OPEN', False)
							passThroughParameters = {}
							for k, v in _request.form.items():
								passThroughParameters[k] = v
							if statsStorage:
								statsCallParameters = _deepcopy(passThroughParameters)
								statsCallParameters['processes'] = 5
								tableName = '%s.%s.%s.%s'%(specificRoutingTable['business_id'],specificRoutingTable['group_id'],specificRoutingTable['tableName'],specificRoutingTable.get('cadence','unknown'))
								statsCallParameters['metric'] = tableName + '.metric'
								statsCallParameters['apiEndPoints'] = specificRoutingTable['opentsdbEndPoints']
								statsCallParameters['influxStorage'] = 0
								for elem in ['encoding', 'field_separator', 'record_separator']:
									statsCallParameters[elem] = specificRoutingTable[elem]
								logger.info('call stats storage server'%())
								detailSteps.append("routing table before stats storage: %s"%(specificRoutingTable))
								if specificRoutingTable.get('statsStorageOTSDB',False): 
									logger.info('form k/v parameters:%s'%(DictionaryAsString(statsCallParameters, maxLength=1000)))
									logger.info('calling statsStorage with data as metrics')
									statsResponse = _post(app.config.get('STATS_STORAGE_SERVICE_ENDPOINT'), data=statsCallParameters, headers=commonHeaders)
									logger.info('stats server envelope response %s'%(statsResponse))
									detailSteps.append('stats server envelope response call: %s'%(statsResponse))
									if statsResponse.status_code >= 200 and statsResponse.status_code <300:
										if statsResponse.json()['status_code'] >= 200 and statsResponse.json()['status_code'] < 300:
											outputPayload['status_code'] = 200
											outputPayload['status_text'] = 'OK'
											outputPayload['version'] = '1'
										else:
											outputPayload['status_code'] = statsResponse.json()['status_code']
											outputPayload['status_text'] = 'ERROR'
											outputPayload['status_detail_text'] += '%s'%(statsResponse.json().get('status_detail_text','StatsStorageSystemError'))
											outputPayload['version'] = '1'
										logger.info('stats server response %s'%(DictionaryAsString(statsResponse.json(), maxLength=100)))
										detailSteps.append('stats server response details: %s'%(DictionaryAsString(statsResponse.json(), maxLength=100)))
									else:
										logger.info('envelop failure for metric expansion stats storage call')
										outputPayload['status_code'] = statsResponse.status_code
										outputPayload['status_text'] = 'ERROR'
										outputPayload['status_detail_text'] += '%s'%(':StatsStorageSystemError')
										outputPayload['version'] = '1'
								else:
									currentMessage = 'statsStorageOTSDB is false or not set'
									logger.info(currentMessage)
									detailSteps.append(currentMessage)
								if specificRoutingTable.get('statsStorageOTSDBTags',False): 

									statsCallParameters['tagsToKeep'] = ",".join(groupColumns)
									statsCallParameters['metric'] = tableName + '.tag'
									logger.info('form k/v parameters:%s'%(DictionaryAsString(statsCallParameters, maxLength=1000)))
									logger.info('statsStorage with tagsToKeep')
									statsResponse = _post(app.config.get('STATS_STORAGE_SERVICE_ENDPOINT'), data=statsCallParameters, headers=commonHeaders)
									logger.info('stats server envelope response as tags %s'%(statsResponse))
									detailSteps.append('stats server envelope response call as tags: %s'%(statsResponse))
									if statsResponse.status_code >= 200 and statsResponse.status_code <300:
										if statsResponse.json()['status_code'] >= 200 and statsResponse.json()['status_code'] < 300:
											outputPayload['status_code'] = 200
											outputPayload['status_text'] = 'OK'
											outputPayload['version'] = '1'
										else:
											outputPayload['status_code'] = statsResponse.json()['status_code']
											outputPayload['status_text'] = 'ERROR'
											outputPayload['status_detail_text'] += '%s'%(statsResponse.json().get('status_detail_text','StatsStorageSystemError'))
											outputPayload['version'] = '1'
										logger.info('stats server response as tags %s'%(DictionaryAsString(statsResponse.json(), maxLength=100)))
										detailSteps.append('stats server response details as tags: %s'%(DictionaryAsString(statsResponse.json(), maxLength=100)))
									else:
										logger.info('envelop failure for opentsdb stats storage wit tagsToKeep set')
										outputPayload['status_code'] = statsResponse.status_code
										outputPayload['status_text'] = 'ERROR'
										outputPayload['status_detail_text'] += '%s'%(':StatsStorageSystemError')
										outputPayload['version'] = '1'
								else:
									currentMessage = 'statsStorageOTSDBTags is false or not set'
									logger.info(currentMessage)
									detailSteps.append(currentMessage)
								if specificRoutingTable.get('statsStorageGrafana', False):
									logger.info('checking romDatasetSize: %d'%(specificRoutingTable['romDatasetSize']))
									if datasetRowCount > app.config.get('INFLUX_MAX_DATASET_SIZE') or specificRoutingTable['romDatasetSize'] > app.config.get('INFLUX_MAX_DATASET_SIZE') :
										logger.warn("dataset size constraint, will not put into influx")
										detailSteps.append('Dataset size constraint, not putting into influx  limit is : %d'%(app.config.get('INFLUX_MAX_DATASET_SIZE')))
									else:
										logger.info("dataset will be put into influx")
										#store into influx
										statsCallParameters.pop('apiEndPoints')
										try: #construct influxEndPoints
											firstEntry = specificRoutingTable['influxEndPoints'].split(',')[0]
											r1 = _compile(r"^https?://(www\.)?")
											firstEntry = r1.sub('',firstEntry)
											influxEntry = firstEntry.split(':')[0]
											influxPort = firstEntry.split(':')[1]
											statsCallParameters['influxStorage'] = 1
											statsCallParameters['apiEndPoints'] = "%s"%(firstEntry)
											logger.info('form k/v parameters:%s'%(DictionaryAsString(statsCallParameters, maxLength=1000)))
											logger.info('make influx stats storage server call'%())
											statsResponse = _post(app.config.get('STATS_STORAGE_SERVICE_ENDPOINT'), data=statsCallParameters, headers=commonHeaders)
											logger.info('influx stats server envelope response %s'%(statsResponse))
											detailSteps.append('influx stats server envelope response call: %s'%(statsResponse))
											if statsResponse.status_code >= 200 and statsResponse.status_code <300:
												if statsResponse.json()['status_code'] >= 200 and statsResponse.json()['status_code'] < 300:
													outputPayload['status_code'] = 200
													outputPayload['status_text'] = 'OK'
													outputPayload['version'] = '1'
												else:
													outputPayload['status_code'] = statsResponse.json()['status_code']
													outputPayload['status_text'] = 'ERROR'
													outputPayload['status_detail_text'] += '%s'%(statsResponse.json().get('status_detail_text','StatsStorageSystemError'))
													outputPayload['version'] = '1'
												logger.info('influx stats server response %s'%(DictionaryAsString(statsResponse.json(), maxLength=100)))
												detailSteps.append('influx stats server response details: %s'%(DictionaryAsString(statsResponse.json(), maxLength=100)))
											else:
												logger.info('envelop failure for influx stats storage call')
												outputPayload['status_code'] = statsResponse.status_code
												outputPayload['status_text'] = 'ERROR'
												outputPayload['status_detail_text'] += '%s'%(':StatsStorageSystemError')
												outputPayload['version'] = '1'
											#create a client connection
											tableNameInflux = tableName + '.input_data'
											client = _InfluxDBClient(influxEntry, influxPort, "", "", tableNameInflux)
											#create the anomaly db if needed
											anomalyTableName = '%s.%s.%s.%s.anomaly'%(specificRoutingTable['business_id'],specificRoutingTable['group_id'],specificRoutingTable['tableName'],specificRoutingTable.get('cadence','unknown'))
											client.create_database(anomalyTableName)
										except Exception as e:
											type_, value_, traceback_ = sys.exc_info()
											#not enough paraeters to construct influx call
											logger.exception('caught exception %s'%(str(e)))
									#this ends else of the block to check for actual data set size
								else:
									currentMessage = 'statsStorageGrafana is false or not set'
									logger.info(currentMessage)
									detailSteps.append(currentMessage)
							else:
								currentMessage = "Stats storage unavailable"
								logger.info(currentMessage)
								detailSteps.append(currentMessage)


							#call anomaly engine
							anomalyEngine = app.config.get('ANOMALY_SERVICE_OPEN', False)
							anomalyResponse = None
							if anomalyEngine:
								if specificRoutingTable.get('runAnomaly', False):
									#put in pause to allow data to finish loading
									pauseTime = max(5,datasetRowCount*app.config.get('ASYNC_WAIT_TIME'))
									logger.info("hold for async to complete: %f seconds"%(pauseTime))
									_sleep(pauseTime)

									anomalyCallParameters = _deepcopy(passThroughParameters)
									anomalyCallParameters['version'] = '1'

									for anomaly_instruction in specificRoutingTable['anomaly_instructions']:
										if type(anomaly_instruction) is not dict:
											raise ValueError('anomaly_instructions key should contain only a list of valid dict as value')
										newSpecificRoutingTable = {**specificRoutingTable, **anomaly_instruction}
										if aggregateColumnCount > app.config.get('MAX_DATASET_COLUMN_SIZE_FOR_ALARM_STORAGE'):
											logger.info('setting write_anomalies_to_db and write_telemetry_to_db to False due to MAX_DATASET_COLUMN_SIZE_FOR_ALARM_STORAGE %d constraint with aggregateColumnCount %d'%(app.config.get('MAX_DATASET_COLUMN_SIZE_FOR_ALARM_STORAGE'), aggregateColumnCount))
											newSpecificRoutingTable['write_anomalies_to_db'] = False
											newSpecificRoutingTable['write_telemetry_to_db'] = False
										anomalyCallParameters['specificRoutingTable'] = _dumps(newSpecificRoutingTable)
										logger.info('anomaly form k/v parameters:%s'%(DictionaryAsString(anomalyCallParameters, maxLength=1000)))
										logger.info('call anomaly server')
										anomalyResponse = _post(app.config.get('ANOMALY_SERVICE_ENDPOINT'), data=anomalyCallParameters, headers=commonHeaders)

										logger.info('anomaly server envelope response %s'%(anomalyResponse))
										detailSteps.append('anomaly server envelope responsead call: %s'%(anomalyResponse))
										if anomalyResponse.status_code >= 200 and anomalyResponse.status_code <300:
											if anomalyResponse.json()['status_code'] >= 200 and anomalyResponse.json()['status_code'] < 300:
												outputPayload['status_code'] = 200
												outputPayload['status_text'] = 'OK'
												outputPayload['version'] = '1'
												if newSpecificRoutingTable.get('returnAnomaly', False):
													logger.info('return entire anomaly response')
													oneCSOutput += anomalyResponse.json()['data']
												else:
													logger.info('return sample of anomaly response')
													oneCSOutput += anomalyResponse.json()['data'][:100]
											else:
												outputPayload['status_code'] = anomalyResponse.json()['status_code']
												outputPayload['status_text'] = 'ERROR'
												outputPayload['version'] = '1'
												outputPayload['status_detail_text'] += '%s'%(anomalyResponse.json().get('status_detail_text','AnomalySystemError'))
											logger.info('anomaly server response %s'%(DictionaryAsString(anomalyResponse.json(), maxLength=100)))
											detailSteps.append('anomaly server response details: %s'%(DictionaryAsString(anomalyResponse.json(), maxLength=100)))
										else:
											logger.info('envelop failure for ad call')
									#loop over anomaly_instruction is done
									#dump full 1cs file here if requested
									outputPayload['data'] = oneCSOutput
									
								else:
									currentMessage = 'runAnomaly is false or not set'
									logger.info(currentMessage)
									detailSteps.append(currentMessage)
							else:
								currentMessage = "Anomaly engine unavailable"
								logger.info(currentMessage)
								detailSteps.append(currentMessage)

							#call the grafana dashboard creation
							writeToGrafana = app.config.get('VISUALIZATION_SERVICE_OPEN', False)
							if writeToGrafana:

								tableName = '%s.%s.%s.%s'%(specificRoutingTable['business_id'],specificRoutingTable['group_id'],specificRoutingTable['tableName'],specificRoutingTable.get('cadence','unknown'))
								dashboardMessage = ''
								if specificRoutingTable.get('createDashboards', False) and datasetRowCount <= app.config.get('INFLUX_MAX_DATASET_SIZE') and specificRoutingTable['romDatasetSize'] <= app.config.get('INFLUX_MAX_DATASET_SIZE'):
									influxDBCreation, influxUrls = VisualizationServerRequest(serverEndpoint = app.config.get('INFLUX_VISUALIZATION_SERVICE_ENDPOINT'), apiEndpoint = specificRoutingTable['influxEndPoints'].split(',')[0], tableName = tableName, host = None, port = None, fields = None, headers = commonHeaders)
									detailSteps.append('influx dashboard call: %s'%(influxDBCreation.text))
									dashboardMessage += "Influx Sandbox Dashboards : %s" %(DictionaryAsString(inputDict = influxUrls, sep="\n\n"))
									dashboardMessage += "\n\n\n"
								else:
									currentMessage = 'no influx dashboard creation'
									logger.info(currentMessage)
									detailSteps.append(currentMessage)
								if specificRoutingTable.get('createDashboardsOTSDB', False):
									logger.info('request creation of OTSDB dashboard creation')
									opentsdbDBCreation, opentsdbUrls = VisualizationServerRequest(serverEndpoint = app.config.get('OPENTSDB_VISUALIZATION_SERVICE_ENDPOINT'), apiEndpoint = specificRoutingTable['opentsdbEndPoints'].split(',')[0], tableName = tableName, host = None, port = None, fields = groupColumns, headers = commonHeaders)
									detailSteps.append('opentsdb dashboard call: %s'%(opentsdbDBCreation.text))
									dashboardMessage += "Opentsdb Sandbox Dashboards : %s" %(DictionaryAsString(inputDict = opentsdbUrls, sep = "\n\n"))
									dashboardMessage += "\n\n\n"
								else:
									currentMessage = 'client did not request opentsdb dashboard creation'
									logger.info(currentMessage)
									detailSteps.append(currentMessage)
							else: #not calling visualization service`as server setting is off
								currentMessage = 'visualization service unavailable'
								logger.info(currentMessage)
								detailSteps.append(currentMessage)
						else:#if len(decodedInputData) > 0: 
							logger.info('decodedInputData is empty')
							outputPayload['status_code'] = 200
							outputPayload['status_text'] = 'OK'
							outputPayload['version'] = '1'
							outputPayload['status_detail_text'] += 'Empty input data '
							detailSteps.append('Supplied data has no records')

					except Exception as e:
						type_, value_, traceback_ = sys.exc_info()
						logger.exception('caught exception %s'%(str(e)))
						
						outputPayload['status_code'] = 501
						outputPayload['status_text'] = 'ERROR'
						outputPayload['version'] = '1'
						outputPayload['status_detail_text'] += 'Error in Orchestrator_mainentry %s'%(str(value_))
						raise(e)

				else:
					#invalid version
					logger.warn('invalid version specified')
					outputPayload['status_code'] = 422 
					outputPayload['status_text'] = 'Invalid API version'
					outputPayload['version'] = '1'
					outputPayload['status_detail_text'] += 'currently version 1 is supported'

		else: #method is not POST
			logger.warn('Only post requests accepted')
			outputPayload['status_code'] = 405
			outputPayload['status_text'] = 'ERROR'
			outputPayload['version'] = '1'
			outputPayload['status_detail_text'] += 'This method accepts POST only'

	except Exception as e:
		type_, value_, traceback_ = sys.exc_info()
		logger.error('caught exception %s'%(str(e)))
		outputPayload['status_code'] = 501
		outputPayload['status_text'] = 'ERROR'
		outputPayload['version'] = '1'
		outputPayload['status_detail_text'] += ':::%s'%(str(e))
	try:
		#build email list from input data
		emailList = []
		clientDashboardMessage = dashboardMessage + "\n********************\nDIAGNOSTICS: " +"\n*****\n\n*****\n".join(detailSteps)
		if specificRoutingTable.get('email_list',None) is not None:
			emailList = specificRoutingTable['email_list'].split(',')
			SendNotification(subject = "Alpha Job Status for %s"%(tableName), message = clientDashboardMessage, notificationList = emailList)
		SendNotification(subject = "Alpha Developer Job Status for %s"%(tableName), message = clientDashboardMessage, notificationList = app.config.get('DEV_EMAIL_LIST', '').split(','))
	except Exception as e:
		type_, value_, traceback_ = sys.exc_info()
		logger.error('caught exception %s'%(str(e)))
		logger.warn('problem sending status email')
		outputPayload['status_detail_text'] += 'Problem sending status email to client team'
	try:
		logger.info('sending email status to dev team')
		message = "%s %s \n\napiResponse = {%s}" %(dashboardMessage, "\n********************\nDIAGNOSTICS: " +"\n*****\n\n*****\n".join(detailSteps), DictionaryAsString(outputPayload, maxLength=100))
		SendNotification(subject = "Alpha Developer Full Job Status for %s"%(tableName), message = message , notificationList = app.config.get('DEV_EMAIL_LIST', '').split(','))
	except Exception as e:
		type_, value_, traceback_ = sys.exc_info()
		logger.error('error sending email to dev team')
		outputPayload['version'] = '1'
		outputPayload['status_detail_text'] += ':::%s'%(str(e))

	outputPayload['status_detail_text'] += "\n\n".join(detailSteps)
	logger.info('leaving orchestrator_mainentry {%s}'%(DictionaryAsString(outputPayload, maxLength=100)))
	return _dumps(outputPayload)

def SendNotification(subject = None, message = None,notificationList = None):

	logger = _getLogger(__name__)
	logger.info("sending notification email %s"%(subject))
	if len(notificationList) == 0:
		logger.info("recipient list empty")
		return
	# Open a plain text file for reading.  For this example, assume that
	# the text file contains only ASCII characters.
	# Create a text/plain message
	msg = _MIMEText(message)

	# me == the sender's email address
	# you == the recipient's email address
	msg['Subject'] = subject
	me = 'AlphaDevTeam@comscore.com' 
	msg['From'] = me
	msg['To'] = (', ').join(notificationList)

	# Send the message via our own SMTP server, but don't include the
	# envelope header.
	s = _SMTP('localhost')
	s.send_message( msg)
	s.quit()

def VisualizationServerRequest(serverEndpoint = None, apiEndpoint = None, tableName = None, host = None, port = None, fields = None, headers = None):
	"""beginning of generic service request routine for visualization service"""
	logger = _getLogger(__name__)
	logger.info("in visualizationServerRequest for %s"%(serverEndpoint))
	regex1 = _compile(r"^https?://(www\.)?")
	message = ""
	dashboardUrls = {}
	r1 = None
	if apiEndpoint is not None:
		firstEntry = apiEndpoint 
		firstEntry = regex1.sub('',firstEntry)
		tsdbEntry = firstEntry.split(':')[0]
		tsdbPort = firstEntry.split(':')[1]
	elif host is not None and port is not None:
		tsdbEntry = host
		tsdbPort = port
	else:
		tsdbEntry = ''
	if len(tsdbEntry) > 0: 
		logger.info('Creating dashboard for %s'%(tsdbEntry))
		logger.info('%s'%(serverEndpoint,))
		tsdbVisualizationCallParameters = {}
		tsdbVisualizationCallParameters['database'] = tableName
		tsdbVisualizationCallParameters['host'] = tsdbEntry
		tsdbVisualizationCallParameters['port'] = tsdbPort
		if fields is not None:
			tsdbVisualizationCallParameters['fields'] = ",".join(fields) 
		logger.info('%s'%(tsdbVisualizationCallParameters))
		logger.info('call visualization server')
		r1 = _post('%s'%(serverEndpoint), data = tsdbVisualizationCallParameters, headers=headers)
		logger.info('%s'%(str(r1)))
		logger.info('%s'%(str(r1.text)))
		if r1.status_code >= 200 and r1.status_code <300:
			j1 = None
			try:
				j1 = r1.json()
				logger.info("json direct %s"%(j1))
			except Exception as e:
				type_, value_, traceback_ = sys.exc_info()
				logger.info("json direct failed %s"%(str(e)))
			try:
				j1 = _loads(r1.text)
				logger.info("json indirect: %s"%(j1))
			except Exception as e:
				type_, value_, traceback_ = sys.exc_info()
				logger.info("json indirect failed %s"%(str(e)))

			message = "Dashboards are ready %s "%(r1.text)
			logger.info(message)
			if j1 is not None:
				try:
					for k,v in r1.json().items():
						if str(k).find('dashboard'):
							#attempt url access
							dashboardUrls[k] = v.get('url',"No Dashboard")
				except Exception as e:
					type_, value_, traceback_ = sys.exc_info()
					logger.info("access dashboards failed %s"%(str(e)))
		else:
			logger.info('envelop failure for visualizationServerRequest')

	else: #len of tsdb entry 0 cannot call 
		logger.warn("invalid call to visualizationServerRequest")
	logger.info("dashboard creation response code %s"%(str(r1)))
	logger.info("dashboard creation response %s"%(str(r1.text)))
	logger.info("dashboard urls %s for %s and %s"%(str(dashboardUrls), serverEndpoint, apiEndpoint))
	return r1, dashboardUrls

def DictionaryAsString(inputDict = None,sep = ',',maxLength = 100):
	return sep.join([f'{key}: {str(value)[:maxLength]}-EV' for key, value in inputDict.items() ] )

def processOTSDBMetaCall(r1,matchingRecord):
	#r1 could be of form: r1 = _get('%s/api/query?start=0&end=now&m=none:metadata.alpha{key=metric_title_registry}'%(registrationEndPoint))
	#matchingRecord could be tableName for metadata.alpha
	#the results of a query are looped through and the value of a tag is 
	#searched for matchingRecord
	logger = _getLogger(__name__)
	logger.info("in processOTSDBMetaCall: searching for %s"%(matchingRecord))
	value = None
	if r1.status_code == 200:
		for elem in r1.json():
			if elem['tags']['value'] == tableName: #found match
				logger.info("%s already exists, increment counter"%(tableName))
				value = float(elem['dps']['1'])
				value += 1
	else:
		logger.info('envelop failure for processOTSDBMetaCall')
	return value

if __name__ == '__main__':
	print('initiate main processing')
	parser = _ArgumentParser()
	parser.add_argument('-port', default=5005, type=int,help='port to run app on')
	args = parser.parse_args()
	z1 = _dumps(globalRoutingService)
	print('before app run, acquired grs as string=%s'%(z1))
	print('before app run, acquired grs as in mem obj=%s'%(globalRoutingService))
	app.run(host = '0.0.0.0',port=args.port, debug=True)
